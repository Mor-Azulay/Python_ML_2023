{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeee97d1-7bb4-4bb9-bd18-d6335a52dc6e",
   "metadata": {},
   "source": [
    "## Effective Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609769c0-3ee2-4810-80f9-f145f035c1bd",
   "metadata": {},
   "source": [
    "## Why Sklearn? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e4497d-163a-4ff5-be1f-627257c67ed6",
   "metadata": {},
   "source": [
    "Scikit-learn (also known as sklearn) is a popular Python library for machine learning and data analysis. There are several reasons to use sklearn:\n",
    "\n",
    "1. Comprehensive library: Scikit-learn includes a wide range of machine learning algorithms, tools for preprocessing data, evaluating models, and model selection.\n",
    "\n",
    "2. Easy to use: Scikit-learn follows a consistent API design that makes it easy to learn and apply. This makes it a great choice for beginners as well as experienced data scientists.\n",
    "\n",
    "3. Interoperability: Scikit-learn is compatible with other Python libraries such as NumPy, pandas, and SciPy. This makes it easy to integrate with existing data processing pipelines.\n",
    "\n",
    "4. Strong documentation: Scikit-learn has extensive documentation and numerous examples, making it easy to understand and apply various machine learning techniques.\n",
    "\n",
    "5. Open-source: Scikit-learn is an open-source project, which means it's free to use, modify, and distribute. This encourages collaboration and continuous improvement by the community.\n",
    "\n",
    "6. Performance: Scikit-learn is built on top of NumPy and SciPy, two libraries optimized for performance. This means that sklearn algorithms are efficient and can handle large datasets.\n",
    "\n",
    "7. Active community: Scikit-learn has a large and active community of users and developers, which means you can easily find support, tutorials, and advice from experts.\n",
    "\n",
    "8. Regular updates: The library is actively maintained and updated, ensuring that it stays up-to-date with the latest advancements in the field of machine learning.\n",
    "\n",
    "Overall, sklearn is a versatile, user-friendly, and powerful library that provides a comprehensive suite of tools for machine learning and data analysis in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad5dd0c-b60a-45a0-a673-a29c3d105f5f",
   "metadata": {},
   "source": [
    "### Why not just scripting with Python and Pandas? Isn't it good enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904051bb-b32a-4b23-8030-316cfe5061ca",
   "metadata": {},
   "source": [
    "While you can use pandas and write your own custom code for preprocessing and pipelining, there are several reasons to consider using sklearn's preprocessing and pipeline objects:\n",
    "\n",
    "1. Consistent API: Scikit-learn provides a consistent API for preprocessing and modeling. This makes it easy to switch between different preprocessing techniques and machine learning algorithms without having to rewrite significant portions of your code.\n",
    "\n",
    "2. Code reusability: Scikit-learn's preprocessing and pipeline objects are designed to be modular and reusable, making it easier to apply the same preprocessing steps to multiple datasets or models, and even share your code with others.\n",
    "\n",
    "3. Model validation: By using sklearn's preprocessing and pipeline objects, you can ensure that the same preprocessing steps are applied consistently to both training and validation/test datasets. This can help prevent common mistakes, such as data leakage, that can occur when preprocessing is done separately.\n",
    "\n",
    "4. Grid search and cross-validation: Scikit-learn's GridSearchCV and cross-validation functions work seamlessly with its preprocessing and pipeline objects. This makes it easy to search for optimal hyperparameters and perform cross-validation, helping you to build more accurate and robust models.\n",
    "\n",
    "5. Maintainability: Using sklearn's preprocessing and pipeline objects can lead to cleaner and more maintainable code. This is because it separates data preprocessing from model training, making it easier to understand and modify your code.\n",
    "\n",
    "6. Performance optimization: Scikit-learn's preprocessing and pipeline objects are optimized for performance, ensuring efficient computation for large datasets. By using sklearn's built-in functions, you can take advantage of these optimizations and avoid potential performance bottlenecks in your custom code.\n",
    "\n",
    "7. Access to advanced techniques: Scikit-learn includes a variety of advanced preprocessing techniques, such as feature scaling, dimensionality reduction, and feature selection, which might not be straightforward to implement on your own.\n",
    "\n",
    "While sklearn's preprocessing and pipeline objects offer many advantages, there can be cases where custom pandas code might be more suitable, such as when you have highly domain-specific preprocessing needs. However, for many common preprocessing tasks and scenarios, using sklearn can save time, reduce the risk of errors, and lead to better overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24285b56-8bab-4d16-a95d-e8eb99595448",
   "metadata": {},
   "source": [
    "## Sklearn popular modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3809b-9812-4eeb-905b-7ca1d3340048",
   "metadata": {},
   "source": [
    "`scikit-learn` offers a wide range of tools for data preprocessing, model training, and evaluation. Some of the most commonly used modules in `scikit-learn` include:\n",
    "\n",
    "1. [`sklearn.datasets`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets): Provides access to built-in datasets for experimentation, such as the famous Iris dataset, breast cancer dataset, or the digits dataset.\n",
    "\n",
    "2. [`sklearn.preprocessing`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing): Offers various functions for data preprocessing, such as scaling, normalization, and encoding categorical features.\n",
    "\n",
    "3. [`sklearn.model_selection`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection): Contains functions for splitting datasets into training and testing sets, cross-validation, and hyperparameter tuning.\n",
    "\n",
    "4. [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics): Provides a wide range of performance metrics for classification, regression, and clustering tasks, such as accuracy, precision, recall, F1-score, and mean squared error.\n",
    "\n",
    "5. [`sklearn.linear_model`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model): Implements various linear models, such as linear regression, logistic regression, and ridge regression.\n",
    "\n",
    "6. [`sklearn.tree`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree): Contains decision tree-based models, including Decision Trees, Random Forests, and Gradient Boosted Trees.\n",
    "\n",
    "7. [`sklearn.ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble): Implements ensemble methods like Random Forests, Gradient Boosting Machines (GBM), and AdaBoost.\n",
    "\n",
    "8. [`sklearn.neighbors`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors): Provides functionality for nearest neighbor-based algorithms, such as K-Nearest Neighbors (KNN) for classification and regression tasks.\n",
    "\n",
    "9. [`sklearn.svm`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm): Implements Support Vector Machine (SVM) models for classification, regression, and outlier detection tasks.\n",
    "\n",
    "10. [`sklearn.cluster`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster): Offers various clustering algorithms, such as K-Means, DBSCAN, and Agglomerative Clustering.\n",
    "\n",
    "11. [`sklearn.decomposition`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition): Contains dimensionality reduction techniques like Principal Component Analysis (PCA), Non-negative Matrix Factorization (NMF), and Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "12. [`sklearn.pipeline`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.pipeline): Provides a convenient way to create pipelines, which are a series of data preprocessing and modeling steps to simplify the overall machine learning workflow.\n",
    "\n",
    "These links will direct you to the relevant documentation pages for each module, where you can learn more about the specific functions and classes they contain.\n",
    "These are just a few examples of the many modules available in `scikit-learn`. Depending on the task at hand, different modules and algorithms may be more appropriate to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da65d3-8be2-45e9-8a64-760cfc417909",
   "metadata": {},
   "source": [
    "In this notebook, we will focus on the `preprocessing`, `model_selection` and `pipeline` modules. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd0f490-d6fa-45a5-b555-e586bac68efa",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd8c302-dd97-46b0-b79b-5fcff50ab435",
   "metadata": {},
   "source": [
    "### Input features types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9187deb-335e-4033-a79e-ae50dfd1e283",
   "metadata": {},
   "source": [
    "Input features, also known as predictors or independent variables, can be of various types and may require proper preprocessing to improve the performance of machine learning models. Here's a brief overview of different types of input features and common issues that can be addressed through preprocessing:\n",
    "\n",
    "1. **Continuous Features**: These are numeric features that can take any value within a range. Examples include age, income, temperature, etc.\n",
    "\n",
    "   Common issues and preprocessing techniques:\n",
    "   - **Scaling**: Continuous features can be on different scales, which might affect the performance of some machine learning algorithms (e.g., distance-based methods like k-NN or gradient-based methods like logistic regression). Techniques like MinMax scaling, standard scaling, or robust scaling can help normalize the feature scales.\n",
    "   - **Outliers**: Outliers can significantly impact the performance of some models. Identifying and handling outliers using methods like IQR-based filtering, Z-score based filtering, or Winsorizing can help improve model performance.\n",
    "\n",
    "2. **Categorical Features**: These are features that can take a limited number of distinct values or categories. Examples include gender, nationality, or color.\n",
    "\n",
    "   Common issues and preprocessing techniques:\n",
    "   - **Encoding**: Machine learning models generally work with numerical values, so it's essential to encode categorical features into numerical representations. Common techniques include one-hot encoding, label encoding, or target encoding.\n",
    "   - **Cardinality**: High cardinality (i.e., a large number of categories) can increase the complexity of the model and lead to overfitting. Techniques to address high cardinality include combining similar categories or using dimensionality reduction methods like PCA.\n",
    "\n",
    "3. **Ordinal Features**: These are categorical features with a natural order. Examples include ratings, education level, or age groups.\n",
    "\n",
    "   Common issues and preprocessing techniques:\n",
    "   - **Encoding**: Ordinal features should be encoded in a way that preserves the natural order. Techniques like label encoding or custom ordinal encoding can be used to represent ordinal features numerically.\n",
    "   - **Imbalanced distribution**: Ordinal features may have an imbalanced distribution across different categories, leading to poor model performance. Techniques like oversampling or undersampling can help balance the distribution.\n",
    "\n",
    "4. **Text Features**: These features consist of textual data, such as user reviews, tweets, or articles.\n",
    "\n",
    "   Common issues and preprocessing techniques:\n",
    "   - **Tokenization**: Text data needs to be broken down into tokens (e.g., words or phrases) that can be used as input features for machine learning models. Tokenization techniques can range from simple whitespace-based tokenization to more advanced methods like stemming or lemmatization.\n",
    "   - **Feature extraction**: Techniques like bag-of-words, TF-IDF, or word embeddings (e.g., Word2Vec, GloVe) can be used to convert text data into numerical representations suitable for machine learning models.\n",
    "\n",
    "5. **Time Series Features**: These features have a temporal component, such as stock prices, weather data, or sensor readings.\n",
    "\n",
    "   Common issues and preprocessing techniques:\n",
    "   - **Seasonality and trends**: Time series data often contains seasonality or trends that can be addressed using techniques like seasonal decomposition or differencing.\n",
    "   - **Feature engineering**: Techniques like lag features, rolling window statistics, or Fourier transformations can help extract relevant information from time series data for machine learning models.\n",
    "\n",
    "Proper preprocessing of input features can significantly improve the performance of machine learning models by addressing issues like missing values, scaling, encoding, and feature engineering. It's essential to understand the characteristics of your data and choose appropriate preprocessing techniques accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf184f9-df2a-4756-b8ff-f8dc5df36fce",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8be04c-5d0a-4333-a639-36b849dc09d5",
   "metadata": {},
   "source": [
    "Regradless of the input feature type, another relativly common issue with data that needs to be addressed in the preprocessing stage is missing values. Missing values can negatively impact the performance of machine learning models. Here's a section on missing values and common techniques to address them:\n",
    "\n",
    "**Missing Values**: Missing values occur when no data is available for a particular feature in some observations. Missing values can arise due to various reasons, such as data entry errors, sensor malfunctions, or information unavailability.\n",
    "\n",
    "preprocessing techniques:\n",
    "\n",
    "- **Identification**: Identifying missing values is the first step in handling them. Missing values can be represented in various ways, such as `NaN`, `None`, or placeholders like `-1` or `9999`. It's crucial to understand how missing values are represented in your dataset.\n",
    "\n",
    "- **Imputation**: Imputation is the process of replacing missing values with estimated values. There are various imputation techniques, depending on the type of feature and the nature of the data. Common imputation methods include:\n",
    "  - **Mean or median imputation**: Replace missing values with the mean or median of the available values for that feature. This method is suitable for continuous features with no significant outliers.\n",
    "  - **Mode imputation**: Replace missing values with the mode (most frequent value) of the available values for that feature. This method is suitable for categorical features.\n",
    "  - **Constant value imputation**: Replace missing values with a constant value. This method is suitable for cases when the missing value itself provides some information (e.g., \"unknown\" or \"not applicable\").\n",
    "  - **Interpolation**: Replace missing values with interpolated values based on nearby data points. This method is suitable for time series or spatial data where the relationship between data points is essential.\n",
    "  - **Advanced imputation methods**: Techniques like k-Nearest Neighbors (k-NN) imputation, regression imputation, or model-based imputation (e.g., using MICE - Multiple Imputation by Chained Equations) can be used to estimate missing values based on other features or underlying patterns in the data.\n",
    "\n",
    "- **Deletion**: In some cases, it might be appropriate to delete instances with missing values, especially when the proportion of missing values is low, and their removal does not introduce significant bias. Deletion can be performed at the row level (i.e., removing instances with missing values) or column level (i.e., removing features with a high proportion of missing values).\n",
    "\n",
    "Handling missing values is a critical step in the preprocessing pipeline, and choosing the appropriate technique depends on the type of feature, the proportion of missing values, and the nature of the data. It's essential to analyze the impact of missing values on your model's performance and experiment with different techniques to find the best approach for your specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb56d08-3ef2-4002-8044-163ef3de9026",
   "metadata": {},
   "source": [
    "### Sklearn Preprocessing module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3da2f-30d9-4c0b-8a47-e938019824cb",
   "metadata": {},
   "source": [
    "#### Transformers overview "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec04d23-40be-4fdf-9a75-f519782ec26e",
   "metadata": {},
   "source": [
    "Scikit-learn's Transformers are a key component of the library's preprocessing and feature extraction capabilities. Transformers are used to modify or transform your dataset's features to create new representations that can be more suitable for machine learning models. They follow a simple and consistent API, which makes it easy to create custom transformers or combine multiple transformers in a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18efffa-8be0-4a5c-9714-78c0d57ebc41",
   "metadata": {},
   "source": [
    "**API**\n",
    "\n",
    "The primary API for a transformer in scikit-learn consists of two main methods:\n",
    "\n",
    "1. `fit(X, y=None)`: This method is used to compute and store any necessary internal parameters or statistics based on the input dataset `X` (and optionally, the target variable `y`). The method returns the transformer object itself (i.e., `self`), enabling method chaining.\n",
    "\n",
    "2. `transform(X)`: This method is used to apply the transformation logic to the input dataset `X`, based on the internal parameters or statistics computed during the `fit` stage. The method returns the transformed dataset, which can be used as input for machine learning models or further preprocessing steps.\n",
    "\n",
    "In addition to these two methods, scikit-learn provides a convenient method called `fit_transform(X, y=None)`, which combines both `fit` and `transform` methods in a single call. In some cases, this method can be more efficient than calling `fit` and `transform` separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96960fc-5f45-4e77-be4f-87d66e95ef61",
   "metadata": {},
   "source": [
    "**Intended Use**\n",
    "\n",
    "Transformers are intended for preprocessing and feature extraction tasks, such as:\n",
    "\n",
    "1. **Data Scaling and Normalization**: Transformers like `StandardScaler`, `MinMaxScaler`, and `RobustScaler` are used to scale and normalize continuous features, making them more suitable for machine learning algorithms sensitive to feature scales.\n",
    "\n",
    "2. **Categorical Encoding**: Transformers like `OneHotEncoder` and `OrdinalEncoder` are used to convert categorical features into numerical representations that can be used as input for machine learning models.\n",
    "\n",
    "3. **Missing Value Imputation**: Transformers like `SimpleImputer` and `KNNImputer` are used to handle missing values in the dataset by replacing them with estimated values based on various strategies.\n",
    "\n",
    "4. **Feature Engineering**: Transformers like `PolynomialFeatures`, `Binarizer`, and `KBinsDiscretizer` are used to create new features from the existing ones, potentially improving the performance of machine learning models.\n",
    "\n",
    "5. **Dimensionality Reduction**: Transformers like `PCA`, `TruncatedSVD`, and `SelectKBest` are used to reduce the number of features in the dataset, making it easier to work with and potentially improving the performance of machine learning models.\n",
    "\n",
    "6. **Text Feature Extraction**: Transformers like `TfidfVectorizer`, `CountVectorizer`, and `HashingVectorizer` are used to convert text data into numerical representations suitable for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a5cef-30a9-4a37-aacd-ac127395bd5b",
   "metadata": {},
   "source": [
    "**Custom Transformers**\n",
    "\n",
    "In addition to the built-in transformers, scikit-learn allows you to create custom transformers by implementing the `fit` and `transform` methods and inheriting from the `BaseEstimator` and `TransformerMixin` classes. This enables you to design domain-specific transformers or combine multiple preprocessing steps in a custom transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df07f4-dad7-4a8f-8690-ec556024b98e",
   "metadata": {},
   "source": [
    "### Examples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cf29c1-c571-4683-aa50-4cc2fa745914",
   "metadata": {},
   "source": [
    "#### Numercial preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02d77e2f-3b51-42e2-b619-96d20e57bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'feature1': [10, 20, 30, 40, 50],\n",
    "    'feature2': [100, 200, 300, 400, 500]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f9364-c2b2-49d5-861d-886d4790ced3",
   "metadata": {},
   "source": [
    "We can easily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f031452-cc85-4330-9f3b-a7b2a72e99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler\n",
    "data_std_scaled_pd = (data - data.mean()) / data.std()\n",
    "\n",
    "# MinMaxScaler\n",
    "data_min_max_scaled_pd = (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "# RobustScaler\n",
    "data_robust_scaled_pd = (data - data.median()) / (data.quantile(0.75) - data.quantile(0.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e200802c-6098-4ae9-93ef-0a4e5765c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "set_config(transform_output=\"pandas\")\n",
    "# StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "data_std_scaled = std_scaler.fit_transform(data)\n",
    "\n",
    "# MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "data_min_max_scaled = min_max_scaler.fit_transform(data)\n",
    "\n",
    "# RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "data_robust_scaled = robust_scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0daeab84-9783-4429-acdf-fb306b47fc22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2\n",
       "0      -1.0      -1.0\n",
       "1      -0.5      -0.5\n",
       "2       0.0       0.0\n",
       "3       0.5       0.5\n",
       "4       1.0       1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_robust_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95248ca9-6ca0-4b12-9fb9-8b673b0cb0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2\n",
       "0      -1.0      -1.0\n",
       "1      -0.5      -0.5\n",
       "2       0.0       0.0\n",
       "3       0.5       0.5\n",
       "4       1.0       1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_robust_scaled_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edecd50-4628-41e7-b4ca-92b84fb1170c",
   "metadata": {},
   "source": [
    "#### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df5ce52b-d84f-4e2d-89a6-e6e220e4081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'feature1': ['A', 'B', 'A', 'C', 'B'],\n",
    "    'feature2': ['X', 'Y', 'X', 'Z', 'Y']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9da4e988-4a52-474f-995b-e3cce1d13a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "data_one_hot_encoded = one_hot_encoder.fit_transform(data)\n",
    "\n",
    "# OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "data_ordinal_encoded = ordinal_encoder.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc65a32b-205c-4bc6-9393-8cb629a99a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder\n",
    "data_one_hot_encoded_pd = pd.get_dummies(data)\n",
    "\n",
    "# OrdinalEncoder\n",
    "data_ordinal_encoded_pd = data.apply(lambda x: x.astype('category').cat.codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cfd2cc77-fd22-498f-9a87-52f2c1bd303c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1_A</th>\n",
       "      <th>feature1_B</th>\n",
       "      <th>feature1_C</th>\n",
       "      <th>feature2_X</th>\n",
       "      <th>feature2_Y</th>\n",
       "      <th>feature2_Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1_A  feature1_B  feature1_C  feature2_X  feature2_Y  feature2_Z\n",
       "0           1           0           0           1           0           0\n",
       "1           0           1           0           0           1           0\n",
       "2           1           0           0           1           0           0\n",
       "3           0           0           1           0           0           1\n",
       "4           0           1           0           0           1           0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_one_hot_encoded.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f84f9792-0668-4411-85f6-d6f052a1d83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1_A</th>\n",
       "      <th>feature1_B</th>\n",
       "      <th>feature1_C</th>\n",
       "      <th>feature2_X</th>\n",
       "      <th>feature2_Y</th>\n",
       "      <th>feature2_Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1_A  feature1_B  feature1_C  feature2_X  feature2_Y  feature2_Z\n",
       "0           1           0           0           1           0           0\n",
       "1           0           1           0           0           1           0\n",
       "2           1           0           0           1           0           0\n",
       "3           0           0           1           0           0           1\n",
       "4           0           1           0           0           1           0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_one_hot_encoded_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd97096-0249-4e01-8ee0-f6b2e3078756",
   "metadata": {},
   "source": [
    "#### Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79d2d291-554a-4e18-9eb2-bdf5135a9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'feature1': [10, np.nan, 30, 40, 50],\n",
    "    'feature2': [100, 200, np.nan, 400, 500]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3222738c-ee66-4357-bb1b-df8d9d32c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# SimpleImputer\n",
    "simple_imputer = SimpleImputer(strategy='mean')\n",
    "data_simple_imputed = simple_imputer.fit_transform(data)\n",
    "\n",
    "# KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=2)\n",
    "data_knn_imputed = knn_imputer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e97d325-09e4-4ac0-a495-ea55932c8c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is already too complicated\n"
     ]
    }
   ],
   "source": [
    "# Pandas equivalint - SimpleImputer\n",
    "data_simple_imputed_pd = data.fillna(data.mean())\n",
    "\n",
    "# Pandas equivalint - KNNImputer\n",
    "print('This is already too complicated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223c3c19-3576-473e-999a-c5d9379120ee",
   "metadata": {},
   "source": [
    "#### Feature space expension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "10633038-0738-44f6-944d-fdd465f4656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7851dfee-6207-4f4c-9d4b-642f72e00945",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=500, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "18db9765-1d1f-4d37-b2dc-dee8178dc0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.750806</td>\n",
       "      <td>0.351308</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.067487</td>\n",
       "      <td>1.006769</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.742971</td>\n",
       "      <td>0.034922</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.611725</td>\n",
       "      <td>-0.631407</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.789013</td>\n",
       "      <td>0.684519</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  target\n",
       "0  -0.750806   0.351308       1\n",
       "1  -0.067487   1.006769       0\n",
       "2  -0.742971   0.034922       1\n",
       "3  -0.611725  -0.631407       0\n",
       "4  -0.789013   0.684519       0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circles_df = pd.DataFrame({'feature_1': X[:,0], 'feature_2': X[:,1], 'target': y})\n",
    "circles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "169f6aa3-85f8-4eab-9a11-32d3cb08b5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_1^2</th>\n",
       "      <th>feature_1 feature_2</th>\n",
       "      <th>feature_2^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.750806</td>\n",
       "      <td>0.351308</td>\n",
       "      <td>0.563709</td>\n",
       "      <td>-0.263764</td>\n",
       "      <td>0.123417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.067487</td>\n",
       "      <td>1.006769</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>-0.067944</td>\n",
       "      <td>1.013584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.742971</td>\n",
       "      <td>0.034922</td>\n",
       "      <td>0.552005</td>\n",
       "      <td>-0.025946</td>\n",
       "      <td>0.001220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.611725</td>\n",
       "      <td>-0.631407</td>\n",
       "      <td>0.374208</td>\n",
       "      <td>0.386248</td>\n",
       "      <td>0.398675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.789013</td>\n",
       "      <td>0.684519</td>\n",
       "      <td>0.622541</td>\n",
       "      <td>-0.540094</td>\n",
       "      <td>0.468566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  feature_1  feature_2  feature_1^2  feature_1 feature_2  feature_2^2\n",
       "0  1.0  -0.750806   0.351308     0.563709            -0.263764     0.123417\n",
       "1  1.0  -0.067487   1.006769     0.004555            -0.067944     1.013584\n",
       "2  1.0  -0.742971   0.034922     0.552005            -0.025946     0.001220\n",
       "3  1.0  -0.611725  -0.631407     0.374208             0.386248     0.398675\n",
       "4  1.0  -0.789013   0.684519     0.622541            -0.540094     0.468566"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolynomialFeatures(degree=2).fit_transform(circles_df[['feature_1', 'feature_2']]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "172d4b31-f623-4b53-807c-a1912ec7d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6482641e-df71-4a8b-bae4-bd0b38d8a730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy without PolynomialFeatures: 0.370\n",
      "Test accuracy with PolynomialFeatures: 0.860\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline without PolynomialFeatures\n",
    "pipeline_no_poly = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(solver='lbfgs', random_state=42))\n",
    "])\n",
    "\n",
    "# Pipeline with PolynomialFeatures\n",
    "pipeline_poly = Pipeline(steps=[\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(solver='lbfgs', random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipelines to the training data\n",
    "pipeline_no_poly.fit(X_train, y_train)\n",
    "pipeline_poly.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipelines on the test data\n",
    "accuracy_no_poly = pipeline_no_poly.score(X_test, y_test)\n",
    "accuracy_poly = pipeline_poly.score(X_test, y_test)\n",
    "\n",
    "print(f\"Test accuracy without PolynomialFeatures: {accuracy_no_poly:.3f}\")\n",
    "print(f\"Test accuracy with PolynomialFeatures: {accuracy_poly:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c3628-22c0-4a0b-a1f3-436a34812aac",
   "metadata": {},
   "source": [
    "Pandas counterparts for some transformers are short and concise. However, there are more advantages to using scikit-learn transformers, especially when working with machine learning models:\n",
    "\n",
    "1. Consistency: scikit-learn transformers are designed to work seamlessly with other scikit-learn components like pipelines and models. This consistency makes it easy to create, understand, and maintain complex preprocessing and modeling workflows.\n",
    "\n",
    "2. Pipelines: scikit-learn transformers can be combined into pipelines, which simplify the process of applying a series of transformations and model fitting. Pipelines make it easier to manage the entire machine learning workflow, including cross-validation and hyperparameter tuning. They also help to avoid common mistakes like leaking information from the test set into the training set.\n",
    "\n",
    "3. Reusability: When working with separate training and test datasets, it's essential to apply the same transformations to both datasets using the parameters learned from the training data. Scikit-learn transformers store these parameters after fitting, allowing you to easily transform new datasets using the same parameters. While it's possible to do this with pandas, it requires additional manual work and can be error-prone.\n",
    "\n",
    "4. Flexibility: scikit-learn transformers often provide more options and flexibility than their pandas counterparts. For example, the `OneHotEncoder` in scikit-learn allows handling of missing values and unknown categories, while `pd.get_dummies()` does not.\n",
    "\n",
    "In summary, while the pandas counterparts for some transformers can be more concise, scikit-learn transformers offer several advantages in terms of consistency, pipeline integration, reusability, flexibility, and compatibility with other tools. These advantages make scikit-learn transformers a better choice for many machine learning workflows, especially when working with separate training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d1987-5749-4ed8-a93e-3f219ff39c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d810ffb5-a647-4889-8c68-4fe0f517d516",
   "metadata": {},
   "source": [
    "#### Preprocessing a mix of categorical and numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cf97f92-b1ea-48f7-8f82-1ef4a6ed74d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9cba455-53a9-4b27-abdc-a6586667c726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66d406-d974-4b1b-9e66-e4e0c3a1da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X = data.drop(columns='Survived')\n",
    "y = data['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bcebe1d-ca1f-4907-b966-cd8f8cb476b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['Sex', 'Embarked']\n",
    "numerical_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "# Create categorical and numerical transformers\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "], remainder='drop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b11ffe66-9c38-431a-8d26-820ac24d37e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 11)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_config(transform_output='default')\n",
    "preprocessor.fit_transform(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7aac0de5-8eaf-49ff-8f38-d92ce74c4d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.810\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a full pipeline with a classifier\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "accuracy = full_pipeline.score(X_test, y_test)\n",
    "print(f\"Test accuracy: {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58faaa2-7b55-4226-8d53-f53d23f378fd",
   "metadata": {},
   "source": [
    "#### Pipelines overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe46d0-35e8-40bc-8245-aba84bf19722",
   "metadata": {},
   "source": [
    "**Pipelines**\n",
    "\n",
    "Transformers can be combined with other transformers or estimators (e.g., classifiers or regressors) in a `Pipeline` object. This enables you to chain multiple preprocessing steps and a final estimator in a single object, simplifying the process of building, evaluating, and deploying machine learning models. Pipelines also ensure that the `fit` and `transform` methods are called in the correct order, and that the internal parameters or statistics are maintained consistently throughout the process.\n",
    "\n",
    "In summary, scikit-learn's Transformers provide a consistent and modular API for a wide range of preprocessing and feature extraction tasks, making it easy to create custom transformers, combine them in pipelines, and improve the performance of machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f17e2a-3b0a-4cc0-9ebb-8547af952aee",
   "metadata": {},
   "source": [
    "The [sklearn.preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html) package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.\n",
    "In general, learning algorithms benefit from standardization of the data set. If some outliers are present in the set, robust scalers or transformers are more appropriate. The behaviors of the different scalers, transformers, and normalizers on a dataset containing marginal outliers is highlighted in Compare the effect of different scalers on data with outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ed8d8-f04e-44ad-bfc2-d8e3b93256df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a557a1-7e83-491f-9cf7-f6549987a9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852deaf7-c25c-4024-b770-b2c6b96a6ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05da5c-aa47-456a-b15c-19620cf69b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d20b79f-6a43-4f85-80b5-0d3c44e30cd8",
   "metadata": {},
   "source": [
    "Numerical vs Categorical features pre-processing:\n",
    "* sklearn.impute\n",
    "* sklearn.preprocessing OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "\n",
    "Text classification:\n",
    "* Kaggle notebook example: https://www.kaggle.com/code/meetnagadia/implementing-text-classification#3.3-Implementing-a-SVM-Model\n",
    "* \n",
    "\n",
    "Feature Selection:\n",
    "* [SelectKBest](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) best\n",
    "* [Recursive feature elimination with cross-validation¶](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py)\n",
    "\n",
    "Pipelines creation:\n",
    "* Feature union example: https://scikit-learn.org/stable/auto_examples/compose/plot_feature_union.html#sphx-glr-auto-examples-compose-plot-feature-union-py\n",
    "* Column Transformer: https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_pipeline_display.html#sphx-glr-auto-examples-miscellaneous-plot-pipeline-display-py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac77d9ee-29ff-4766-b170-caf9fdac6aa3",
   "metadata": {},
   "source": [
    "Import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d60fc98-82f9-4995-9029-ef0b5586d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb287f-b557-4b88-ad8a-91223108566d",
   "metadata": {},
   "source": [
    "Load dataset (Iris dataset as an example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "076e8e87-9c18-481c-8293-7b4c358c9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X = data['data']\n",
    "y = data['target']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eaffe4-8740-4054-847c-1ad8abc2a0e3",
   "metadata": {},
   "source": [
    "Split dataset into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db362314-0d52-4936-b35b-3c1dbad5f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea61245b-b609-4219-962e-2b0f803bd74e",
   "metadata": {},
   "source": [
    "Define ColumnTransformer for preprocessing (assuming categorical and numerical features are already identified):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1506803a-92dd-40d7-a450-5fa5decce1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['numerical_feature_1', 'numerical_feature_2']),\n",
    "        ('cat', OneHotEncoder(), ['categorical_feature_1', 'categorical_feature_2'])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0b48f-ebc2-4364-86c3-225048849ff2",
   "metadata": {},
   "source": [
    "Create pipelines for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec264f5-e4c5-4aca-967b-18a2c60e4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "svc_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(random_state=42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032d32b2-b4a0-4a01-8ca7-360a330dd23c",
   "metadata": {},
   "source": [
    "Set up hyperparameter search grids for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d58d20-6f90-4664-89c4-98d7ce7570f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    'classifier__n_estimators': [10, 50, 100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "}\n",
    "\n",
    "logreg_param_grid = {\n",
    "    'classifier__C': np.logspace(-3, 3, 7),\n",
    "    'classifier__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "svc_param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10, 100],\n",
    "    'classifier__kernel': ['linear', 'rbf']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d9d3f-f6a4-4278-8f02-92098605652b",
   "metadata": {},
   "source": [
    "Perform hyperparameter search using GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1add7c-9f9a-470a-bcc5-cb656082f30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idodavid/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Specifying the columns using strings is only supported for pandas DataFrames",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0mall_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-08ee15dfe158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrf_grid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_param_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrf_grid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogreg_grid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogreg_param_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlogreg_grid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \"\"\"\n\u001b[1;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_validate_remainder\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_column_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mremaining_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mall_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             raise ValueError(\"Specifying the columns using strings is only \"\n\u001b[0m\u001b[1;32m    377\u001b[0m                              \"supported for pandas DataFrames\")\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Specifying the columns using strings is only supported for pandas DataFrames"
     ]
    }
   ],
   "source": [
    "rf_grid_search = GridSearchCV(rf_pipeline, rf_param_grid, cv=5, verbose=1, n_jobs=-1)\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "logreg_grid_search = GridSearchCV(logreg_pipeline, logreg_param_grid, cv=5, verbose=1, n_jobs=-1)\n",
    "logreg_grid_search.fit(X_train, y_train)\n",
    "\n",
    "svc_grid_search = GridSearchCV(svc_pipeline, svc_param_grid, cv=5, verbose=1, n_jobs=-1)\n",
    "svc_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d15a0c9-54f1-4598-ae05-152305b15856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5570ddc-24d7-4e87-84b1-bddb502f44d8",
   "metadata": {},
   "source": [
    "# Introducing the `set_output` API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5278daa-d727-4789-b6e6-f00640f0832a",
   "metadata": {},
   "source": [
    "This example will demonstrate the `set_output` API to configure transformers to\n",
    "output pandas DataFrames. `set_output` can be configured per estimator by calling\n",
    "the `set_output` method or globally by setting `set_config(transform_output=\"pandas\")`.\n",
    "For details, see\n",
    "[SLEP018](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep018/proposal.html)_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79caded8-8039-4cb8-a193-bdea5b06671a",
   "metadata": {},
   "source": [
    "First, we load the iris dataset as a DataFrame to demonstrate the `set_output` API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b4a9b4-0a36-43e6-a8a7-cd5d7df45f0a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "60                 5.0               2.0                3.5               1.0\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "8                  4.4               2.9                1.4               0.2\n",
       "93                 5.0               2.3                3.3               1.0\n",
       "106                4.9               2.5                4.5               1.7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(as_frame=True, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaceb05-d10a-488a-8fe0-8b5240960721",
   "metadata": {},
   "source": [
    "To configure an estimator such as :class:`preprocessing.StandardScaler` to return\n",
    "DataFrames, call `set_output`. This feature requires pandas to be installed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e6e68c-3162-4967-a9dd-e310243a41cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5af82c2-7f86-41c7-8892-6d8fb79752cc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.894264</td>\n",
       "      <td>0.798301</td>\n",
       "      <td>-1.271411</td>\n",
       "      <td>-1.327605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.244466</td>\n",
       "      <td>-0.086944</td>\n",
       "      <td>-1.327407</td>\n",
       "      <td>-1.459074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.660797</td>\n",
       "      <td>1.462234</td>\n",
       "      <td>-1.271411</td>\n",
       "      <td>-1.327605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.894264</td>\n",
       "      <td>0.576989</td>\n",
       "      <td>-1.159419</td>\n",
       "      <td>-0.933197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-0.427329</td>\n",
       "      <td>-1.414810</td>\n",
       "      <td>-0.039497</td>\n",
       "      <td>-0.275851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "39          -0.894264          0.798301          -1.271411         -1.327605\n",
       "12          -1.244466         -0.086944          -1.327407         -1.459074\n",
       "48          -0.660797          1.462234          -1.271411         -1.327605\n",
       "23          -0.894264          0.576989          -1.159419         -0.933197\n",
       "81          -0.427329         -1.414810          -0.039497         -0.275851"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8318ba-8f5b-4134-8690-463e5ec3b245",
   "metadata": {},
   "source": [
    "`set_output` can be called after `fit` to configure `transform` after the fact.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a810ebf2-4cb3-4d64-93d5-e868a22b23ae",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default output type: ndarray\n",
      "Configured pandas output type: DataFrame\n"
     ]
    }
   ],
   "source": [
    "scaler2 = StandardScaler()\n",
    "\n",
    "scaler2.fit(X_train)\n",
    "X_test_np = scaler2.transform(X_test)\n",
    "print(f\"Default output type: {type(X_test_np).__name__}\")\n",
    "\n",
    "scaler2.set_output(transform=\"pandas\")\n",
    "X_test_df = scaler2.transform(X_test)\n",
    "print(f\"Configured pandas output type: {type(X_test_df).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d0e47d-ba6e-4843-ad25-b552f8e5f5dc",
   "metadata": {},
   "source": [
    "In a :class:`pipeline.Pipeline`, `set_output` configures all steps to output\n",
    "DataFrames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51ccf0dc-3e6b-4430-a220-215adf22bbbe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;selectpercentile&#x27;, SelectPercentile(percentile=75)),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;selectpercentile&#x27;, SelectPercentile(percentile=75)),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectPercentile</label><div class=\"sk-toggleable__content\"><pre>SelectPercentile(percentile=75)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('selectpercentile', SelectPercentile(percentile=75)),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "clf = make_pipeline(\n",
    "    StandardScaler(), SelectPercentile(percentile=75), LogisticRegression()\n",
    ")\n",
    "clf.set_output(transform=\"pandas\")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312316e4-0655-42a7-8b30-0c39cdd6062d",
   "metadata": {},
   "source": [
    "Each transformer in the pipeline is configured to return DataFrames. This\n",
    "means that the final logistic regression step contains the feature names of the input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1afb85c-b348-40e1-8ac5-b24472c720cb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sepal length (cm)', 'petal length (cm)', 'petal width (cm)'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf[-1].feature_names_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a40660-aa4c-4db1-9c2e-8e755b23f227",
   "metadata": {},
   "source": [
    "Next we load the titanic dataset to demonstrate `set_output` with\n",
    ":class:`compose.ColumnTransformer` and heterogenous data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80126641-eb99-4b98-bec9-1c6297a2f184",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml(\n",
    "    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1902e6a4-0485-4b11-9193-8cea4394fb41",
   "metadata": {},
   "source": [
    "The `set_output` API can be configured globally by using :func:`set_config` and\n",
    "setting `transform_output` to `\"pandas\"`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "785ab6b4-9467-4e59-b691-07d288159bcd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8048780487804879"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "num_pipe = make_pipeline(SimpleImputer(), StandardScaler())\n",
    "num_cols = [\"age\", \"fare\"]\n",
    "ct = ColumnTransformer(\n",
    "    (\n",
    "        (\"numerical\", num_pipe, num_cols),\n",
    "        (\n",
    "            \"categorical\",\n",
    "            OneHotEncoder(\n",
    "                sparse_output=False, drop=\"if_binary\", handle_unknown=\"ignore\"\n",
    "            ),\n",
    "            [\"embarked\", \"sex\", \"pclass\"],\n",
    "        ),\n",
    "    ),\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "clf = make_pipeline(ct, SelectPercentile(percentile=50), LogisticRegression())\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d370dc64-f7f9-4582-a1e0-787d13cd06c6",
   "metadata": {},
   "source": [
    "With the global configuration, all transformers output DataFrames. This allows us to\n",
    "easily plot the logistic regression coefficients with the corresponding feature names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da0adade-a79b-4199-b165-c0efec609b34",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "log_reg = clf[-1]\n",
    "coef = pd.Series(log_reg.coef_.ravel(), index=log_reg.feature_names_in_)\n",
    "coef.sort_values().plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57acedc9-9772-4ec5-abca-de8912bf69de",
   "metadata": {},
   "source": [
    "This resets `transform_output` to its default value to avoid impacting other\n",
    "examples when generating the scikit-learn documentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8ed0df0-9eb5-4099-b853-c8464a1c4db9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "set_config(transform_output=\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c4cdd-5082-467e-afe7-33b943e1c2be",
   "metadata": {},
   "source": [
    "When configuring the output type with :func:`config_context` the\n",
    "configuration at the time when `transform` or `fit_transform` are\n",
    "called is what counts. Setting these only when you construct or fit\n",
    "the transformer has no effect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd759391-c3ed-4291-a60b-4610a7563b23",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import config_context\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6605835d-7ddf-4bd5-a605-110881795ba6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.828276</td>\n",
       "      <td>3.759535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>-0.549971</td>\n",
       "      <td>0.127120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.490490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>-0.271667</td>\n",
       "      <td>-0.487106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.704603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           age      fare\n",
       "11   -0.828276  3.759535\n",
       "1147 -0.549971  0.127120\n",
       "865        NaN -0.490490\n",
       "860  -0.271667 -0.487106\n",
       "1172       NaN  0.704603"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with config_context(transform_output=\"pandas\"):\n",
    "    # the output of transform will be a Pandas DataFrame\n",
    "    X_test_scaled = scaler.transform(X_test[num_cols])\n",
    "X_test_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc20d769-f088-4551-9d39-bc9f1e7247d4",
   "metadata": {},
   "source": [
    "outside of the context manager, the output will be a NumPy array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c7166ec-1145-42aa-9ee7-4cb7002ea477",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.82827555,  3.75953504],\n",
       "       [-0.54997108,  0.12711996],\n",
       "       [        nan, -0.49048971],\n",
       "       [-0.2716666 , -0.48710555],\n",
       "       [        nan,  0.70460313]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled = scaler.transform(X_test[num_cols])\n",
    "X_test_scaled[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855025d-7ab4-4b6e-aea6-e38882405f4a",
   "metadata": {},
   "source": [
    "## AutoML (Automated Machine Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448c07c-15f2-4641-a0e8-023b8382eb04",
   "metadata": {},
   "source": [
    "AutoML, or Automated Machine Learning, is a field of machine learning that aims to automate the process of building, selecting, and tuning machine learning models. It is designed to help users with limited ML expertise quickly build models for various tasks, reducing the time and effort required in the model development process. AutoML typically automates tasks like data preprocessing, feature engineering, model selection, and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeddf19-a295-4747-a1ee-53d9f0e63416",
   "metadata": {},
   "source": [
    "Some of the most popular AutoML tools and libraries include:\n",
    "\n",
    "* Google Cloud AutoML: A suite of machine learning products offered by Google Cloud, which allows users to train custom ML models with minimal expertise. It supports various tasks like image classification, natural language processing, and tabular data analysis.\n",
    "\n",
    "* AutoGluon: A Python library built on top of the Gluon library, focusing on AutoML. AutoGluon simplifies the process of applying machine learning models to real-world problems by automating data preprocessing, feature engineering, model selection, and hyperparameter tuning.\n",
    "\n",
    "* TPOT: The Tree-based Pipeline Optimization Tool (TPOT) is an AutoML library that uses genetic programming to optimize machine learning pipelines. It automates the process of selecting the best model, feature selection, and hyperparameter tuning.\n",
    "\n",
    "* Auto-Sklearn: Auto-Sklearn is an automated machine learning toolkit built on top of the popular Scikit-learn library. It automates the selection of machine learning models and hyperparameter tuning using Bayesian optimization, meta-learning, and ensemble learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc7f79-ef8f-4e06-80b1-8e63960df177",
   "metadata": {},
   "source": [
    "### AutoGluon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41dd8c0-a5d3-4dac-b5b5-fcc7e1a1a1f3",
   "metadata": {},
   "source": [
    "\n",
    "AutoGluon aims to simplify the process of applying machine learning models to real-world problems by automating the steps like data preprocessing, feature engineering, model selection, and hyperparameter tuning.\n",
    "AutoGluon is designed to be easy-to-use, allowing users with minimal ML expertise to leverage state-of-the-art models for their specific tasks. It provides an efficient and flexible API for various machine learning tasks, including:\n",
    "\n",
    "1. Tabular data: AutoGluon can automatically train models for classification and regression tasks on structured datasets (e.g., CSV files or Pandas DataFrames).\n",
    "2. Text data: It supports natural language processing tasks, such as text classification and sentiment analysis.\n",
    "3. Image data: AutoGluon can be used for image classification, object detection, and other computer vision tasks.\n",
    "4. Time-series data: It can handle time-series forecasting tasks for both univariate and multivariate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a90614-6eba-4219-8883-6a44c1afcb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b27e21fe-2a52-4a27-b7a9-ad75375ed62e",
   "metadata": {},
   "source": [
    "### Auto-Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9fd68d-0205-4727-ad63-912e23c6dfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-ml-kernel",
   "language": "python",
   "name": "python-ml-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
